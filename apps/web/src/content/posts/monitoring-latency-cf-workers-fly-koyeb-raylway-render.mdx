---
title:
  "Monitoring latency: Cloudflare Workers vs Fly vs Koyeb vs Railway vs Render"
description:
  We have deployed a simple Hono server on Cloudflare workers, Fly, Koyeb,
  Railway and Render cloud providers to compare the latency of each of them
  using OpenStatus.
author:
  name: Thibault Le Ouay Ducasse
  url: https://twitter.com/thibaultleouay
publishedAt: 2024-02-18
image: /assets/posts/monitoring-latency/all-hosting-providers.png
---

We wanted to test the latency of [Cloudflare Workers](#cloudflare-workers),
[Fly](#flyio), [Koyeb](#koyeb), [Railway](#railway) and [Render](#render) using
OpenStatus.

<Tweet id="1752762168041439731" />

It's a good way to dogfood our own product and improve it.

For this test, we used a basic [Hono](https://hono.dev) server that returns a
simple text response.

```
const app = new Hono(); app.use("*", logger());

app.use("\*", poweredBy());

app.get('/', (c) => {
  return c.text('Just return the desired http status code, e.g. /404 ðŸ¤¯ \nhttps://www.openstatus.dev')
})
```

You can find the code [here](https://github.com/openstatusHQ/status-code]), itâ€™s
open source ðŸ˜‰.

We deployed the application on the cheapest or free tier offered by each
provider.

We monitored the latency of each provider using OpenStatus. We pinged them every
**10 minutes** from our 6 locations located in Amsterdam, Sao Paulo, Hong Kong,
Ashburn, Johannesburg, and Sydney.

Let's analyze the data from the past two weeks.

## Cloudflare workers

<div className="grid grid-cols-2 gap-4 sm:grid-cols-3 md:grid-cols-5">
  <MetricsCard title="uptime" value={100} suffix="%" variant="positive" />
  <MetricsCard title="fails" value={0} suffix="#" variant="negative" />
  <MetricsCard title="total pings" value={10956} suffix="#" variant="info" />
  <div className="hidden md:col-span-2 md:block" />
  <MetricsCard title="avg" value={182} suffix="ms" />
  <MetricsCard title="p75" value={138} suffix="ms" />
  <MetricsCard title="p90" value={695} suffix="ms" />
  <MetricsCard title="p95" value={778} suffix="ms" />
  <MetricsCard title="p99" value={991} suffix="ms" />
</div>

<div className="mt-4">
  <SimpleChart
    staticFile="/assets/posts/monitoring-latency/cloudflare.json"
    caption="Cloudflare avg. latency between 04. Feb and 18. Feb 2024 aggregated in a 1h window."
  />
</div>

### Timing metrics

| Region | DNS (ms) | Connection (ms) | TLS Handshake (ms) | TTFB (ms) | Transfert (ms) |
| ------ | -------- | --------------- | ------------------ | --------- | -------------- |
| AMS    | 17       | 2               | 17                 | 27        | 0              |
| GRU    | 38       | 2               | 13                 | 28        | 0              |
| HKG    | 19       | 2               | 13                 | 29        | 0              |
| IAD    | 24       | 1               | 14                 | 30        | 0              |
| JNB    | 123      | 168             | 182                | 185       | 0              |
| SYD    | 51       | 1               | 11                 | 25        | 0              |

We can notice the Johannesburg is taking around 10 times more times than the
other monitors.

### Headers

From the Cloudflare request we can get the location of the workers that handle
the request, with `Cf-ray` in the headers response.

| Checker region | Workers region | number of request |
| -------------- | -------------- | ----------------- |
| HKG            | HKG            | 1831              |
| SYD            | SYD            | 1831              |
| AMS            | AMS            | 1831              |
| IAD            | IAD            | 1831              |
| GRU            | GRU            | 1791              |
| GRU            | GIG            | 40                |
| JNB            | AMS            | 741               |
| JNB            | MUC            | 4                 |
| JNB            | HKG            | 5                 |
| JNB            | SIN            | 6                 |
| JNB            | NRT            | 8                 |
| JNB            | EWR            | 10                |
| JNB            | CDG            | 82                |
| JNB            | FRA            | 276               |
| JNB            | LHR            | 699               |
| JNB            | AMS            | 741               |

We can see all the request from JNB is never routed to a nearby datacenter.

Apart from the strange routing error in Johannesburg, Cloudflare workers are
fast worldwide.

We have not experienced any cold start issues.

## Fly.io

<div className="grid grid-cols-2 gap-4 sm:grid-cols-3 md:grid-cols-5">
  <MetricsCard title="uptime" value={100} suffix="%" variant="positive" />
  <MetricsCard title="fails" value={0} suffix="#" variant="negative" />
  <MetricsCard title="total pings" value={10952} suffix="#" variant="info" />
  <div className="hidden md:col-span-2 md:block" />
  <MetricsCard title="avg" value={1471} suffix="ms" />
  <MetricsCard title="p75" value={1514} suffix="ms" />
  <MetricsCard title="p90" value={1555} suffix="ms" />
  <MetricsCard title="p95" value={1626} suffix="ms" />
  <MetricsCard title="p99" value={2547} suffix="ms" />
</div>

<div className="mt-4">
  <SimpleChart
    staticFile="/assets/posts/monitoring-latency/fly.json"
    caption="Fly avg. latency between 04. Feb and 18. Feb 2024 aggregated in a 1h window."
  />
</div>

### Timing metrics

| Region | DNS (ms) | Connection (ms) | TLS Handshake (ms) | TTFB (ms) | Transfert (ms) |
| ------ | -------- | --------------- | ------------------ | --------- | -------------- |
| AMS    | 6        | 1               | 8                  | 1469      | 0              |
| GRU    | 5        | 0               | 4                  | 1431      | 0              |
| HKG    | 4        | 0               | 5                  | 1473      | 0              |
| IAD    | 3        | 0               | 5                  | 1470      | 0              |
| JNB    | 24       | 0               | 5                  | 1423      | 0              |
| SYD    | 3        | 0               | 3                  | 1489      | 0              |

The DNS is fast. We are attempting to connect to a region in the same data
center as our checker, but our machine's cold start is slowing us down, leading
to the high TTFB.

Hereâ€™s our config for Fly.io:

```
app = 'statuscode'
primary_region = 'ams'

[build]
  dockerfile = "./Dockerfile"

[http_service]
  internal_port = 3000
  force_https = true
  auto_stop_machines = true
  auto_start_machines = true
  min_machines_running = 0
  processes = ['app']

[[vm]]
  cpu_kind = 'shared'
  cpus = 1
  memory_mb = 256
```

Our primary region is Amsterdam, and the fly instances are getting paused after
a period of inactivity.

We only have our monitor hitting our machine.

Itâ€™s slow but according to the logs our machine start in `1.513643778s`

```
2024-02-14T11:24:16.107 proxy[286560ea703108] ams [info] Starting machine

2024-02-14T11:24:16.322 app[286560ea703108] ams [info] [ 0.035736] PCI: Fatal: No config space access function found

2024-02-14T11:24:16.533 app[286560ea703108] ams [info] INFO Starting init (commit: bfa79be)...

2024-02-14T11:24:16.546 app[286560ea703108] ams [info] INFO Preparing to run: `/usr/local/bin/docker-entrypoint.sh bun start` as root

2024-02-14T11:24:16.558 app[286560ea703108] ams [info] INFO [fly api proxy] listening at /.fly/api

2024-02-14T11:24:16.565 app[286560ea703108] ams [info] 2024/02/14 11:24:16 listening on [fdaa:3:2ef:a7b:10c:3c9a:5b4:2]:22 (DNS: [fdaa::3]:53)

2024-02-14T11:24:16.611 app[286560ea703108] ams [info] $ bun src/index.ts

2024-02-14T11:24:16.618 runner[286560ea703108] ams [info] Machine started in 460ms

2024-02-14T11:24:17.621 proxy[286560ea703108] ams [info] machine started in 1.513643778s

2024-02-14T11:24:17.628 proxy[286560ea703108] ams [info] machine became reachable in 7.03669ms
```

#### OpenStatus Prod metrics

<div className="grid grid-cols-2 gap-4 sm:grid-cols-3 md:grid-cols-5">
  <MetricsCard title="uptime" value={100} suffix="%" variant="positive" />
  <MetricsCard title="fails" value={0} suffix="#" variant="negative" />
  <MetricsCard title="total pings" value={12076} suffix="#" variant="info" />
  <div className="hidden md:col-span-2 md:block" />
  <MetricsCard title="avg" value={61} suffix="ms" />
  <MetricsCard title="p75" value={67} suffix="ms" />
  <MetricsCard title="p90" value={164} suffix="ms" />
  <MetricsCard title="p95" value={198} suffix="ms" />
  <MetricsCard title="p99" value={327} suffix="ms" />
</div>

We use Fly.io in production, and the machine never sleeps, yielding much better
results.

## Koyeb

<div className="grid grid-cols-2 gap-4 sm:grid-cols-3 md:grid-cols-5">
  <MetricsCard title="uptime" value={100} suffix="%" variant="positive" />
  <MetricsCard title="fails" value={0} suffix="#" variant="negative" />
  <MetricsCard title="total pings" value={10955} suffix="#" variant="info" />
  <div className="hidden md:col-span-2 md:block" />
  <MetricsCard title="avg" value={539} suffix="ms" />
  <MetricsCard title="p75" value={738} suffix="ms" />
  <MetricsCard title="p90" value={881} suffix="ms" />
  <MetricsCard title="p95" value={1013} suffix="ms" />
  <MetricsCard title="p99" value={1525} suffix="ms" />
</div>

<div className="mt-4">
  <SimpleChart
    staticFile="/assets/posts/monitoring-latency/koyeb.json"
    caption="Koyeb avg. latency between 04. Feb and 18. Feb 2024."
  />
</div>

### Timing metrics

| Region | DNS (ms) | Connection (ms) | TLS Handshake (ms) | TTFB (ms) | Transfert (ms) |
| ------ | -------- | --------------- | ------------------ | --------- | -------------- |
| AMS    | 50       | 2               | 17                 | 107       | 0              |
| GRU    | 139      | 65              | 75                 | 407       | 0              |
| HKG    | 48       | 2               | 13                 | 321       | 0              |
| IAD    | 35       | 1               | 12                 | 129       | 0              |
| JNB    | 298      | 1               | 11                 | 720       | 0              |
| SYD    | 97       | 1               | 10                 | 711       | 0              |

### Headers

If we look at the request header none of our request are cached They contains
the `cf-cache-status: dynamic` Koyeb edge layer is handle by Cloudflare

https://www.koyeb.com/blog/building-a-multi-region-service-mesh-with-kuma-envoy-anycast-bgp-and-mtls

Our request are following this route

```
Cf workers -> koyeb Global load balancer -> koyeb backend
```

Letâ€™s see where did we hit the cf workers

| Checker region | Workers region | number of request |
| -------------- | -------------- | ----------------- |
| AMS            | AMS            | 1866              |
| GRU            | GRU            | 504               |
| GRU            | IAD            | 38                |
| GRU            | MIA            | 688               |
| GRU            | EWR            | 337               |
| GRU            | CIG            | 299               |
| HKG            | HKG            | 1866              |
| IAD            | IAD            | 1866              |
| JNB            | JNB            | 1861              |
| JNB            | AMS            | 1                 |
| SYD            | SYD            | 1866              |

Koyeb Global Load Balancer region we hit:

| Checker region | Koyeb Global Load Balancer | number of request |
| -------------- | -------------------------- | ----------------- |
| AMS            | FRA1                       | 1866              |
| GRU            | WAS1                       | 1866              |
| HKG            | SIN1                       | 1866              |
| IAD            | WAS1                       | 1866              |
| JNB            | PAR1                       | 4                 |
| JNB            | SIN1                       | 1864              |
| JNB            | FRA1                       | 1                 |
| JNB            | SIN1                       | 1866              |

I have deployed our app in the Frankfurt data-center.

## Railway

<div className="grid grid-cols-2 gap-4 sm:grid-cols-3 md:grid-cols-5">
  <MetricsCard title="uptime" value={99.991} suffix="%" variant="positive" />
  <MetricsCard title="fails" value={1} suffix="#" variant="negative" />
  <MetricsCard title="total pings" value={10955} suffix="#" variant="info" />
  <div className="hidden md:col-span-2 md:block" />
  <MetricsCard title="avg" value={381} suffix="ms" />
  <MetricsCard title="p75" value={469} suffix="ms" />
  <MetricsCard title="p90" value={653} suffix="ms" />
  <MetricsCard title="p95" value={661} suffix="ms" />
  <MetricsCard title="p99" value={850} suffix="ms" />
</div>

<div className="mt-4">
  <SimpleChart
    staticFile="/assets/posts/monitoring-latency/railway.json"
    caption="Railway avg. latency between 04. Feb and 18. Feb 2024 aggregated in a 1h window."
  />
</div>

### Timing metrics

| Region | DNS (ms) | Connection (ms) | TLS Handshake (ms) | TTFB (ms) | Transfert (ms) |
| ------ | -------- | --------------- | ------------------ | --------- | -------------- |
| AMS    | 9        | 21              | 18                 | 158       | 0              |
| GRU    | 14       | 115             | 127                | 178       | 0              |
| HKG    | 8        | 45              | 54                 | 225       | 0              |
| IAD    | 7        | 2               | 14                 | 65        | 0              |
| JNB    | 18       | 193             | 178                | 319       | 0              |
| SYD    | 21       | 108             | 105                | 280       | 0              |

### Headers

The headers don't provide any information.

Railway is using Google Cloud Platform. Itâ€™s the only service that does not
allow us to pick a specific region on the free plan. Our test app will be
located to `us-west1` Portland, Oregon. We can see that the latency is the
lowest in IAD.

By default our app did not scale down to 0. It was always running. We don't have
any cold start.

## Render

<div className="grid grid-cols-2 gap-4 sm:grid-cols-3 md:grid-cols-5">
  <MetricsCard title="uptime" value={99.89} suffix="%" variant="positive" />
  <MetricsCard title="fails" value={12} suffix="#" variant="negative" />
  <MetricsCard title="total pings" value={10946} suffix="#" variant="info" />
  <div className="hidden md:col-span-2 md:block" />
  <MetricsCard title="avg" value={451} suffix="ms" />
  <MetricsCard title="p75" value={447} suffix="ms" />
  <MetricsCard title="p90" value={591} suffix="ms" />
  <MetricsCard title="p95" value={707} suffix="ms" />
  <MetricsCard title="p99" value={902} suffix="ms" />
</div>

<div className="mt-4">
  <SimpleChart
    staticFile="/assets/posts/monitoring-latency/render.json"
    caption="Render avg. latency between 04. Feb and 18. Feb 2024 aggregated in a 1h window."
  />
</div>

### Timing metrics

| Region | DNS (ms) | Connection (ms) | TLS Handshake (ms) | TTFB (ms) | Transfert (ms) |
| ------ | -------- | --------------- | ------------------ | --------- | -------------- |
| AMS    | 20       | 2               | 7                  | 107       | 0              |
| GRU    | 61       | 2               | 6                  | 407       | 0              |
| HKG    | 76       | 2               | 6                  | 321       | 0              |
| IAD    | 15       | 1               | 5                  | 129       | 0              |
| JNB    | 36       | 161             | 167                | 720       | 0              |
| SYD    | 103      | 1               | 4                  | 711       | 0              |

### Headers

The headers don't provide any information.

We have deployed our app in the Frankfurt datacenter.

According to the Render docs, the free tier will shut down the service after 15
minutes of inactivity. However, our app is being accessed by a monitor every 10
minutes. We should never scale down to 0.

```
Render spins down a Free web service that goes 15 minutes without receiving inbound traffic. Render spins the service back up whenever it next receives a request to process.
```

I think the failure are from the cold start of our app. We have a default
timeout of 30s and the render app takes up to 50s to start. We might have hit
inflection point between cold and warm.

## Conclusion

If you are only looking from a latency perspective, Cloudflare Workers is the
best choice. Itâ€™s fast worldwide and we havenâ€™t encountered any cold start
issues. But it comes with its own limitations.

Picking a cloud provider is not only about latency. Itâ€™s also about the DX. We
have to take into account the ease of use and the pricing of each of them.

For us we are using Fly.io in production, and we are happy with it.

### Bonus Vercel

We haven't included Vercel in this test, but the code needs to be structure
differently. We are running a test to compare the latency of Vercel Edge, Vercel
Serverless.

If you want to monitor your own website, create an account on
[OpenStatus](https://www.openstatus.dev/app/sign-up?ref=blog-monitoring).
